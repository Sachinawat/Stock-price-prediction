# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
# from sklearn.metrics import mean_squared_error
# from statsmodels.tsa.arima.model import ARIMA # type: ignore
# from statsmodels.tsa.statespace.sarimax import SARIMAX
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, GRU, Dense
# import warnings

# warnings.filterwarnings("ignore")

# # ==========================================
# # üõ†Ô∏è DATA PREPARATION
# # ==========================================
# def create_sequences(data, seq_length):
#     xs, ys = [], []
#     for i in range(len(data) - seq_length):
#         x = data[i:(i + seq_length)]
#         y = data[i + seq_length]
#         xs.append(x)
#         ys.append(y)
#     return np.array(xs), np.array(ys)

# # ==========================================
# # üß† MODEL DEFINITIONS
# # ==========================================

# def run_arima(train, test, future_steps):
#     # Order (p,d,q) hardcoded for speed, in prod use auto_arima
#     model = ARIMA(train, order=(5,1,0)) 
#     model_fit = model.fit()
    
#     # Validate
#     predictions = model_fit.forecast(steps=len(test))
#     error = np.sqrt(mean_squared_error(test, predictions))
    
#     # Predict Future
#     full_model = ARIMA(np.concatenate([train, test]), order=(5,1,0)).fit()
#     future_pred = full_model.forecast(steps=future_steps)
    
#     return error, future_pred[-1]

# def run_sarima(train, test, future_steps):
#     # SARIMA adds seasonality
#     model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))
#     model_fit = model.fit(disp=False)
    
#     predictions = model_fit.forecast(steps=len(test))
#     error = np.sqrt(mean_squared_error(test, predictions))
    
#     full_model = SARIMAX(np.concatenate([train, test]), order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)).fit(disp=False)
#     future_pred = full_model.forecast(steps=future_steps)
    
#     return error, future_pred[-1]

# def run_deep_learning(model_type, train_data, test_data, scaler, future_steps, seq_length=60):
#     # Prepare Data
#     X_train, y_train = create_sequences(train_data, seq_length)
    
#     # Build Model
#     model = Sequential()
#     if model_type == 'LSTM':
#         model.add(LSTM(50, return_sequences=False, input_shape=(seq_length, 1)))
#     else: # GRU
#         model.add(GRU(50, return_sequences=False, input_shape=(seq_length, 1)))
    
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
    
#     # Train (Verbose=0 hides logs)
#     model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=0)
    
#     # Validate
#     # (Complex logic to predict test set step-by-step omitted for brevity, doing simple batch predict)
#     # We will skip strict Validation loop for DL here to keep speed, assume last knowns
    
#     # Recursive Future Prediction
#     last_sequence = np.concatenate([train_data, test_data])[-seq_length:]
#     current_seq = last_sequence.reshape((1, seq_length, 1))
    
#     # To measure error fairly, we'd need to simulate the test set. 
#     # For this demo, we will assign a slightly lower weight to DL unless data is massive.
#     # Let's do a quick validation on the last known point.
#     val_pred = model.predict(X_train[-1].reshape(1, seq_length, 1), verbose=0)
#     error = np.sqrt(mean_squared_error([y_train[-1]], val_pred[0]))
    
#     # Forecast Future
#     curr = current_seq.copy()
#     pred_value = 0
    
#     for _ in range(future_steps):
#         pred = model.predict(curr, verbose=0)[0][0]
#         pred_value = pred
#         # Update sequence: remove first, add new pred
#         new_step = np.array([[[pred]]])
#         curr = np.append(curr[:, 1:, :], new_step, axis=1)
        
#     # Inverse transform
#     final_price = scaler.inverse_transform([[pred_value]])[0][0]
#     real_error = scaler.inverse_transform([[error]])[0][0]
    
#     return real_error, final_price

# # ==========================================
# # üèÜ THE ARENA (MANAGER)
# # ==========================================
# def race_models_and_predict(df, days_ahead):
#     """
#     Runs all models, compares accuracy, returns best prediction.
#     """
#     data = df['Close'].values
#     train_size = int(len(data) * 0.9) # 90% Train, 10% Test
#     train, test = data[:train_size], data[train_size:]
    
#     results = {}
    
#     print(f"   ... Training ARIMA")
#     try:
#         err, pred = run_arima(train, test, days_ahead)
#         results['ARIMA'] = {'error': err, 'prediction': pred}
#     except: results['ARIMA'] = {'error': 9999, 'prediction': 0}

#     print(f"   ... Training SARIMA")
#     try:
#         err, pred = run_sarima(train, test, days_ahead)
#         results['SARIMA'] = {'error': err, 'prediction': pred}
#     except: results['SARIMA'] = {'error': 9999, 'prediction': 0}

#     # Prepare DL Data Scaling
#     scaler = MinMaxScaler(feature_range=(0, 1))
#     scaled_data = scaler.fit_transform(data.reshape(-1, 1))
#     train_scaled = scaled_data[:train_size]
#     test_scaled = scaled_data[train_size:]

#     print(f"   ... Training LSTM (Deep Learning)")
#     try:
#         err, pred = run_deep_learning('LSTM', train_scaled, test_scaled, scaler, days_ahead)
#         results['LSTM'] = {'error': err, 'prediction': pred}
#     except Exception as e: 
#         print(e)
#         results['LSTM'] = {'error': 9999, 'prediction': 0}

#     print(f"   ... Training GRU (Deep Learning)")
#     try:
#         err, pred = run_deep_learning('GRU', train_scaled, test_scaled, scaler, days_ahead)
#         results['GRU'] = {'error': err, 'prediction': pred}
#     except: results['GRU'] = {'error': 9999, 'prediction': 0}

#     # Find Winner
#     best_model = min(results, key=lambda x: results[x]['error'])
    
#     return best_model, results




































import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import warnings

warnings.filterwarnings("ignore")

# ==========================================
# üß† ENHANCED AI MODELS
# ==========================================

def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        xs.append(data[i:(i + seq_length)])
        ys.append(data[i + seq_length])
    return np.array(xs), np.array(ys)

def run_deep_learning(model_type, train_data, test_data, scaler, future_steps):
    # INCREASED LOOKBACK: Looks at past 90 days (Quarterly trend) to predict next day
    SEQ_LENGTH = 90 
    
    # Combined dataset for sequence creation
    full_dataset = np.concatenate((train_data, test_data))
    
    X_train, y_train = create_sequences(train_data, SEQ_LENGTH)
    
    model = Sequential()
    # LSTM with more neurons for 10 years of data
    model.add(LSTM(100, return_sequences=False, input_shape=(SEQ_LENGTH, 1)))
    model.add(Dropout(0.2)) # Prevents overfitting on large data
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    
    # Train longer (10 epochs) because we have 10 years of data
    model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=0)
    
    # Predict Future
    curr_seq = full_dataset[-SEQ_LENGTH:].reshape(1, SEQ_LENGTH, 1)
    pred_value = 0
    
    for _ in range(future_steps):
        pred = model.predict(curr_seq, verbose=0)[0][0]
        pred_value = pred
        new_step = np.array([[[pred]]])
        curr_seq = np.append(curr_seq[:, 1:, :], new_step, axis=1)
        
    final_price = scaler.inverse_transform([[pred_value]])[0][0]
    
    # Simple validation score (Last known point)
    val_pred = model.predict(X_train[-1].reshape(1, SEQ_LENGTH, 1), verbose=0)
    error = abs(val_pred[0][0] - y_train[-1][0])
    
    return error, final_price

def race_models_and_predict(df, days_ahead):
    data = df['Close'].values
    
    # Use 95% for training (keep recent 5% for validation)
    split = int(len(data) * 0.95)
    train, test = data[:split], data[split:]
    
    results = {}

    # 1. ARIMA (Standard Trend)
    try:
        model = ARIMA(train, order=(5,1,0)).fit()
        full_model = ARIMA(data, order=(5,1,0)).fit()
        pred = full_model.forecast(steps=days_ahead)[-1]
        results['ARIMA'] = {'error': 1, 'prediction': pred} # Placeholder error
    except: pass

    # 2. LSTM (Deep Learning - Long Term Memory)
    try:
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(data.reshape(-1, 1))
        train_scaled = scaled_data[:split]
        test_scaled = scaled_data[split:]
        
        err, pred = run_deep_learning('LSTM', train_scaled, test_scaled, scaler, days_ahead)
        results['LSTM'] = {'error': err, 'prediction': pred}
    except Exception as e: print(e)

    # Pick Best
    best_model = 'LSTM' if 'LSTM' in results else 'ARIMA'
    return best_model, results